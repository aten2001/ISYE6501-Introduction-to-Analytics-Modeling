---
title: "Homework 4"
author: "Chen Yi-Ju(Ernie)"
date: "2020/6/9"
output: 
  #html_document: default
  pdf_document:
    latex_engine: xelatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Question 9.1
###  Question 9.1
Using the same crime data set uscrime.txt as in Question 8.2, apply Principal Component Analysis and then create a regression model using the first few principal components. Specify your new model in terms of the original variables (not the principal components), and compare its quality to that of your solution to Question 8.2.

### Answer: Using an estimation using both 4 and 5  PCAs(assumption based on scree plot), the results were worse than the original regression. This may be due to overtraining on the original regression.


```{r ,echo = TRUE, message = FALSE, warning = FALSE}
setwd("D:/ernie/self-study/GTxMicroMasters/Introduction to Analytics Modeling/week4/homework")
library(tidyverse)
crime <- read.table("uscrime.txt",header = T) %>% 
  data.frame()
```

```{r}
#original model from 8.2 with 9 variables
model <- lm (data = crime , Crime ~ Ed + Ineq + LF + M + M.F +Po1 + Pop + Prob  + Time )
summary(model)
```

```{r}
pca <- prcomp(formula =  ~  So + Ed + Ineq + LF + M + M.F +Po1 + Pop + Prob  + Time  + Po2 + NW + U1 +U2 + Wealth , data = crime , scale = T)
pca
```

```{r}
#Plotting Scree plot
# Kaiser eigenvalue-greater-than-one rule
Scree <- plot(pca,         
              type="line", 
              main="Scree Plot for crime factors")%>%
              abline(h=1, col="blue")
```

```{r}
# Calculate  and Plot the variances and proportion of variances 

var <- pca$sdev^2
propvar <- var/sum(var)
```

```{r}
#plotting
por.explained <- plot(propvar, xlab = "Principle component" , ylab = "Porpotion explained" , type = "s")
acc.por.explained <- plot(cumsum(propvar) , xlab = "Principle component" , ylab = " AccamulatedPorpotion explained" , type = "o")

```

```{r}
#choose 4 new variables(variance > 1)
pca.chosen <- pca$x[, 1:4]
pca.chosen
```

```{r}
#Combining PCAs  with crime 
new.crime <- cbind(pca.chosen, crime[,16]) %>%
  data.frame()
colnames(new.crime)[5] <- "Crime"
new.model <- lm ( Crime ~ PC1 + PC2 + PC3 + PC4 ,data = new.crime )
summary(new.model)
```

```{r}
#Transforming new data back to original variable
PCAs <- new.model$coefficients[2:5]
intercept <- new.model$coefficients[1]
original <- pca$rotation[,1:4] %*% PCAs
original
PCAs
```

### These are the variables used, expressed in  original form.
```{r}
# un-scaling data

origi.var <- original/sapply(crime[,1:15],sd)
origi.inter <-  intercept - sum(original*sapply(crime[,1:15],mean)/sapply(crime[,1:15],sd))

origi.var
```


```{r}
#Trying with 5 PCAs
#choose 4 new variables(variance > 1)
pca.chosen2 <- pca$x[, 1:5]
pca.chosen2
```

```{r}
#Combining PCAs  with crime 
new.crime2 <- cbind(pca.chosen2, crime[,16]) %>%
  data.frame()
colnames(new.crime2)[6] <- "Crime"
new.model2 <- lm ( Crime ~ PC1 + PC2 + PC3 + PC4 + PC5 ,data = new.crime2 )
summary(new.model2)
```



## Question 10.1
### Using the same crime data set uscrime.txt as in Questions 8.2 and 9.1, find the best model you can using(a) a regression tree model, and (b) a random forest model.


```{r , echo = TRUE, message = FALSE, warning = FALSE}
#Question 10.1
library(rpart)
library(rpart.plot)
library(randomForest)
library(caret)
library(modelr)
```

```{r}
#CART
# set up train and testing split
train <- createDataPartition(crime$Crime, p = .85, list = F)
# set up test and train datasets
crime.train <- crime[train,]
crime.test <- crime[-train,]
# check splits
dim(crime.train); dim(crime.test)
```

```{r}
#Regression Tree
crime.tree <- train(
  Crime ~ .,
  data = crime.train,
  method = 'rpart',
  trControl = trainControl(method = 'boot_all', number = 10),
  metric = 'RMSE'
)

```

```{r}
crime.tree$finalModel
```

```{r}
prp(crime.tree$finalModel)
```

```{r}
#Random Forest
crime.forest <- train(
  Crime ~ .,
  data = crime.train,
  method = 'rf',
  trControl = trainControl(method = 'boot_all', number = 10),
  metric = 'RMSE')
crime.forest$finalModel
```

```{r}
#Testing
crime.res1 <- crime.test %>%
  add_predictions(., crime.tree) %>%
  select('observations' = Crime, pred) %>%
  as.data.frame()
crime.res2 <- crime.test %>%
  add_predictions(., crime.forest) %>%
  select('observations' = Crime, pred) %>%
  as.data.frame()
```

```{r}
crime.res1
```

```{r}
crime.res2
```

```{r}
postResample(obs = crime.res1$observations, pred = crime.res1$pred)
```

```{r}
postResample(obs = crime.res2$observations, pred = crime.res2$pred)
```



## Question 10.2
### Describe a situation or problem from your job, everyday life, current events, etc., for which a logistic regression model would be appropriate. List some (up to 5) predictors that you might use.

### I would consider logisitc regession useful in predicting customer behavior on EC sites. The results would be buy (0) and don't buy(1). As for predictors, age, occupation, time spent on site would be considered good predictors.

## Question 10.3
### Using the GermanCredit data set germancredit.txt , use logistic regression to find a good predictive model for whether credit applicants are good credit risks or not.

```{r}
set.seed(101)
credit <- read.table("germancredit.txt", header = FALSE)
str(credit)
credit$V21[credit$V21==1] <- 0
credit$V21[credit$V21==2] <- 1
```


```{r}
#Dividing data
credit.part <- createDataPartition(credit$V21, times = 1, p = 0.7, list=FALSE)
head(credit.part)
credit.train <- credit[credit.part,] 
credit.valid <- credit[-credit.part,]
```

```{r}
#model
credit.log <- glm(V21 ~ ., data = credit.train, family=binomial(link="logit"))
summary(credit.log)
```

```{r}
#Confusion Matrix
creditPredict <- predict(credit.log, newdata=credit.valid[,-21], type="response")
Confusion.mat <- table(credit.valid$V21, round(creditPredict))
```

### We can see that although sensitivity is quite high, Specifity isn't

```{r}
Sensitivity <- Confusion.mat[1,1]/sum(Confusion.mat[1,])
Sensitivity
```

```{r}
Specfitity <-Confusion.mat[2,2]/sum(Confusion.mat[2,])
Specfitity
```

### Then we change the threshhold

```{r}
#setting second thresh hold
threshold <- 0.7
thres <- as.matrix(table(round(creditPredict > threshold), credit.valid$V21))
names(dimnames(thres)) <- c("Predicted", "Observed")
thres
```

### Below are the results for a different threshold, there is an obvious rise in specifity but also a slight loss in sensitivity. 
```{r}
Sensitivity2 <- thres[1,1]/sum(thres[1,])
Sensitivity2
```

```{r}
Specfitity2 <-thres[2,2]/sum(thres[2,])
Specfitity2
```

