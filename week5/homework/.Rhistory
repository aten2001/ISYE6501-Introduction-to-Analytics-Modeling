library(ggthemes)
head(mpg)
head(mpg)
rm(list = ())
rm(list = ls())
#Histogram of hwy mpg values:
pl <- ggplot(mpg, x = hwy)
pl + geom_histogram()
#Histogram of hwy mpg values:
pl <- ggplot(mpg, aes(x = hwy))
pl + geom_histogram()
pl + geom_histogram(color = 'red')
pl + geom_histogram(fill = 'red')
pl + geom_histogram(fill = 'red', alpha = 0.1)
pl + geom_histogram(fill = 'red', alpha = 0.6)
pl + geom_histogram(binwidth = 40, fill = 'red', alpha = 0.6)
pl + geom_histogram(binwidth = 4, fill = 'red', alpha = 0.6)
pl + geom_histogram(binwidth = 1, fill = 'red', alpha = 0.6)
pl + geom_histogram(binwidth = 3, fill = 'red', alpha = 0.6)
pl + geom_histogram(binwidth = 3, fill = 'red', alpha = 0.5)
pl + geom_histogram(binwidth = 3, fill = 'red', alpha = 0.4)
pl + geom_histogram(binwidth = 2.5, fill = 'red', alpha = 0.4)
pl + geom_histogram(binwidth = 2, fill = 'red', alpha = 0.4)
pl + geom_histogram(binwidth = 1, fill = 'red', alpha = 0.4)
pl + geom_histogram(binwidth = 1.5, fill = 'red', alpha = 0.4)
theme_set(theme_base())
#Histogram of hwy mpg values:
pl <- ggplot(mpg, aes(x = hwy))
pl + geom_histogram(binwidth = 1.5, fill = 'red', alpha = 0.4)
theme_set(theme_basic())
#Histogram of hwy mpg values:
pl <- ggplot(mpg, aes(x = hwy))
pl + geom_histogram(binwidth = 1.5, fill = 'red', alpha = 0.4)
theme_set(theme_classic())
#Histogram of hwy mpg values:
pl <- ggplot(mpg, aes(x = hwy))
pl + geom_histogram(binwidth = 1.5, fill = 'red', alpha = 0.4)
theme_set(theme_gray())
#Histogram of hwy mpg values:
pl <- ggplot(mpg, aes(x = hwy))
pl + geom_histogram(binwidth = 1.5, fill = 'red', alpha = 0.4)
head(mpg)
#Barplot of car counts per manufacturer with color fill defined by cyl count
pl2 <- ggplot(mpg, aes(x = manufacturer))
pl + geom_bar()
pl2 + geom_bar()
pl2 + geom_bar(aes(color = factor(cyl)))
pl + geom_histogram(binwidth = 1.5, fill = 'red', alpha = 0.4) + theme_gray()
pl2 + geom_bar(aes(color = factor(cyl)))
pl2 + geom_bar(aes(color = factor(cyl))) + theme_classic()
pl2 + geom_bar(aes(fill = factor(cyl))) + theme_classic()
#Switch now to use the txhousing dataset that comes with ggplot2
head(txhousing)
#Create a scatterplot of volume versus sales.
#Afterwards play around with alpha and color arguments to clarify information.
pl3 <- ggplot(txhousing, aes(x = sales, y = volume))
print(pl3)
pl3 + geom_point()
pl3 + geom_point(aes(color= factor(volume)))
pl3 + geom_point()
pl3 + geom_point()
pl3 + geom_point(aes(color = sales))
pl3 + geom_point(color = 'blue',aes(color = sales))
pl3 + geom_point(aes(color = sales))
pl3 + geom_point(size= 5, aes(color = sales))
pl3 + geom_point(size= 2, aes(color = sales))
pl3 + geom_point(size= 1.5, aes(color = sales))
pl3 + geom_point(size= 1.5, aes(color = sales)) + scale_colour_gradient(high='red',low = "blue")
pl3 + geom_point(size= 1.5, aes(color = sales)) + scale_colour_gradient("blue")
pl3 + geom_point(size= 1.5, aes(color = sales)) + scale_colour_gradient(high = 'blue')
pl3 + geom_point(size= 1.5, aes(color = sales, alpha = volume)) + scale_colour_gradient(high = 'blue')
pl3 + geom_point(size= 1.5, aes(color = sales, alpha = desc(volume))) + scale_colour_gradient(high = 'blue')
pl3 + geom_point(size= 1.5, aes(color = sales, alpha = volume)) + scale_colour_gradient(high = 'blue')
pl4 <-pl3 + geom_point(size= 1.5, aes(color = sales, alpha = volume)) + scale_colour_gradient(high = 'blue')
print(pl4)
#Add a smooth fit line to the scatterplot from above.
#Hint: You may need to look up geom_smooth()
pl4 + geom_smooth()
help("geom_smooth")
#Add a smooth fit line to the scatterplot from above.
#Hint: You may need to look up geom_smooth()
pl4 + geom_smooth(color = "red")
pl4 <-pl3 + geom_point(size= 2, aes(color = sales, alpha = volume)) + scale_colour_gradient(high = 'blue')
print(pl4)
#Add a smooth fit line to the scatterplot from above.
#Hint: You may need to look up geom_smooth()
pl4 + geom_smooth(color = "red")
help("geom_smooth")
pl4 <-pl3 + geom_point(size= 1.7, aes(color = sales, alpha = volume)) + scale_colour_gradient(high = 'blue')
print(pl4)
#Add a smooth fit line to the scatterplot from above.
#Hint: You may need to look up geom_smooth()
pl4 + geom_smooth(color = "red")
help("geom_smooth")
pl2 + geom_bar(aes(fill = factor(cyl))) + theme_gdocs()
pl4 <-pl3 + geom_point(color = 'blue', alpha =0.5)
print(pl4)
total = 0
for i in 1:n
for (i in 1:n){
total = total + i
}
total = 0
sum_total <- function(n){
for (i in 1:n){
total = total + i
}
}
sum_total(10)
sum_total(10)
print (sum_total(10))
View(sum_total)
sum_total <- function(n){
list_1 <- 1:n
sum (list_1)
}
print (sum_total(10))
list_1 <- 1:n
list_1 <- 1:10
rm(list = ls())
total = 0
sum_total <- function(n){
list_1 <- 1:n
sum (list_1)
}
print (sum_total(10))
print (sum_total(10000000))
print (sum_total(10000))
print (sum_total(999))
print (sum_total(4))
df <- read.csv("D:/ernie/self-study/ML-marathon/3rd-ML100Days/dataset/Home Credit Default Risk")
df <- read.csv("D:/ernie/self-study/ML-marathon/3rd-ML100Days/dataset/Home Credit Default Risk/application_train.csv")
df <- read.csv("D:/ernie/self-study/ML-marathon/3rd-ML100Days/dataset/Home Credit Default Risk/application_train.csv")
head(df)
summary(df)
row(df)
nrow(df)
ncol(df)
df[0:]
df[0,]
df[,0]
df
stats = 2
stats = 2
Python = 2
calc/lin = 1
R/book = 1
df = [11.4,12.5,10.2,10.8,11.6]
df = vector[11.4,12.5,10.2,10.8,11.6]
df = vector(11.4,12.5,10.2,10.8,11.6)
df = (11.4,12.5,10.2,10.8,11.6)
df <-  c(11.4,12.5,10.2,10.8,11.6)
mean (df)
var (df)
rm(list=ls())
df <-  c(11.4,12.5,10.2,10.8,11.6)
mean (df)
var (df)
df <- vector(10.6,12.3,11.2,11.4,10.0)
df <- [10.6,12.3,11.2,11.4,10.0]
df <- c(10.6,12.3,11.2,11.4,10.0)
sd(df)
var(df)
root(0.75)
sqrt(0.75)
mean(df)
installed.packages()
installed.packages("tidyverse")
help("installed.packages")
install.packages("tidyverse")
install.packages("reshape")
updateR
install.packages("installr")
library(installr)
updateR()
updateR()
install.packages(c("caTools", "ggplot2", "kernlab", "kknn"))
tinytex:::is_tinytex()
291200 ^(1/8)
(1.15*0.92*1.2*0.9)^(1/4)
The results, represented in a matrix and in a graph show that the accuracy would be at its highest if k = 1, giving and accuracy of 72.4%(approx.)<br>
Therefore, using k = 1, I use the model again to test on the test data.<br>
```{r, echo = TRUE, message = FALSE, warning = FALSE}
set.seed(101)
knn_model4=kknn(R1~., train = train.data,test = test.data,k=1, scale = T)
sum(knn_model4$fitted.values == test.data$R1) / nrow(test.data)
```
The result is better what was expected, giving an accuracy of 81.8%(approx.)
tinytex::install_tinytex()
knitr::opts_chunk$set(echo = TRUE)
setwd("D:/ernie/self-study/GTxMicroMasters/Introduction to Analytics Modeling/week 1")
credit <- read.table("credit_card_data-headers.txt" , header = T)
head(credit)
library(kernlab)
library(kknn)
library(ggplot2)
library(dplyr)
library(caTools)
set.seed(101)
model.1 <- ksvm(x = as.matrix(credit[,1:10]),
y = as.factor(credit[,11]),
type = "C-svc" ,
scaled = TRUE ,
kernel = "vanilladot" ,
C = 100)
model.1
a <- colSums(model.1@xmatrix[[1]]*model.1@coef[[1]])
a0 <- model.1@b
pred1 <- predict(model.1,credit[,1:10])
res1 <- sum(pred1 == credit [,11]) / nrow(credit)
setwd("D:/ernie/self-study/GTxMicroMasters/Introduction to Analytics Modeling/week 1")
credit <- read.table("credit_card_data-headers.txt" , header = T)
head(credit)
library(kernlab)
library(kknn)
library(ggplot2)
library(dplyr)
library(caTools)
set.seed(101)
model.1 <- ksvm(x = as.matrix(credit[,1:10]),
y = as.factor(credit[,11]),
type = "C-svc" ,
scaled = TRUE ,
kernel = "vanilladot" ,
C = 100)
a <- colSums(model.1@xmatrix[[1]]*model.1@coef[[1]])
a0 <- model.1@b
model.1
a
a0
setwd("D:/ernie/self-study/GTxMicroMasters/Introduction to Analytics Modeling/week 1")
credit <- read.table("credit_card_data-headers.txt" , header = T)
head(credit)
library(kernlab)
library(kknn)
library(ggplot2)
library(dplyr)
library(caTools)
set.seed(101)
model.1 <- ksvm(x = as.matrix(credit[,1:10]),
y = as.factor(credit[,11]),
type = "C-svc" ,
scaled = TRUE ,
kernel = "vanilladot" ,
C = 100)
a <- colSums(model.1@xmatrix[[1]]*model.1@coef[[1]])
a0 <- model.1@b
pred1 <- predict(model.1,credit[,1:10])
res1 <- sum(pred1 == credit [,11]) / nrow(credit)
res1
#choosing best model : c =1 ~100
set.seed(101)
test.c <- list(1:100)
acc <- data.frame(matrix(ncol = 2, nrow = 100))
names(acc) <- c("c","accuracy")
for (i in test.c){
model <- ksvm(x = as.matrix(credit[,1:10]),
y = as.factor(credit[,11]),
type = "C-svc" ,
scaled = TRUE ,
kernel = "vanilladot" ,
C = i)
pred <- predict(model,credit[,1:10])
res.0 <- sum(pred1 == credit [,11]) / nrow(credit)
acc[i,1] <- i
acc[i,2] <- res.0
}
svm.plt <-ggplot(acc, aes(x = c , y = accuracy)) + geom_point() + geom_line(lty = "dotted" , color = "red")
svm.plt
#2.2.2
#Using other non-linear models
#Radial Basis kernel "Gaussian"
set.seed(101)
model.2 <- ksvm(x = as.matrix(credit[,1:10]),
y = as.factor(credit[,11]),
type = "C-svc" ,
scaled = TRUE ,
kernel = "rbfdot" ,
C = 100)
b <- colSums(model.2@xmatrix[[1]]*model.2@coef[[1]])
b0 <- model.1@b
pred2 <- predict(model.2,credit[,1:10])
res2 <- sum(pred2 == credit [,11]) / nrow(credit)
#Polynomial kernel
set.seed(101)
model.3 <- ksvm(x = as.matrix(credit[,1:10]),
y = as.factor(credit[,11]),
type = "C-svc" ,
scaled = TRUE ,
kernel = "polydot" ,
C = 100)
c <- colSums(model.3@xmatrix[[1]]*model.3@coef[[1]])
c0 <- model.3@b
pred3 <- predict(model.3,credit[,1:10])
res3 <- sum(pred3 == credit [,11]) / nrow(credit)
# Hyperbolic tangent kernel
set.seed(101)
model.4 <- ksvm(x = as.matrix(credit[,1:10]),
y = as.factor(credit[,11]),
type = "C-svc" ,
scaled = TRUE ,
kernel = "tanhdot" ,
C = 100)
d <- colSums(model.4@xmatrix[[1]]*model.4@coef[[1]])
d0 <- model.4@b
pred4 <- predict(model.4,credit[,1:10])
res4 <- sum(pred4 == credit [,11]) / nrow(credit)
#summaring up results
pred.list <- c(res1,res2,res3,res4)
kernel.list <- c("Linear","Radial Basis" ,"Polynomial" ,"Hyperbolic tangent")
result.df <- data.frame(kernel.list ,pred.list)
print(result.df)
R1 <-credit[,11]
pred5<- rep(0,(nrow(credit)))
set.seed(101)
for (i in 1:nrow(credit)){
#making sure that i won't use it self
knn.model=kknn(R1~., credit[-i,],credit[i,],k=1, scale = T)
pred5[i]<- as.integer(fitted(knn.model)+0.5)
}
res5 = sum(pred5 == R1) / nrow(credit)
res5
knn.df <- data.frame(matrix(nrow = 30, ncol = 2))
colnames(knn.df) <- c("k" , "accuracy")
set.seed(101)
for(n in 1:30){
for (i in 1:nrow(credit)){
knn_model=kknn(R1~., credit[-i,],credit[i,],k=n, scale = T)
pred5[i] <- as.integer(fitted(knn_model)+0.5)
res.00 <- sum(pred5 == R1) / nrow(credit)
knn.df[n,1]<- n
knn.df[n,2]<- res.00
}
}
knn.plt <-ggplot(knn.df, aes(x = k , y = accuracy)) + geom_point() + geom_line(lty = "dotted" , color = "red")
knn.plt
head(knn.df[order(-knn.df["accuracy"]),])
#3.1
#a
#k-fold Cross validation
acc2 <- data.frame(matrix(nrow = 30 ,ncol = 2))
names(acc2) <- c("k" , "accuracy")
set.seed(101)
for (i in 1:30){
knn_model2 <- cv.kknn(R1~ ., credit , kcv = 10 , k = i, scale = T)
pred6 <- round(knn_model2[[1]][,2])
res6 <- sum(pred6 == credit [,11]) / nrow(credit)
acc2[i,1] <- i
acc2[i,2] <- res6
}
head(acc2[order(-acc2["accuracy"]),])
knn.plt2 <-ggplot(acc2, aes(x = k , y = accuracy)) + geom_point() + geom_line(lty = "dotted" , color = "red")
knn.plt2
#best model
head(acc2[order(-acc2["accuracy"]),])
#splitting data
train.index <- sample(nrow(credit),nrow(credit) * 0.7)
train.data <-  credit[train.index,]
remaining_data <- credit[-train.index,]
vad.index <- sample(nrow(remaining_data),nrow(remaining_data) * 0.5)
vad.data <- remaining_data[vad.index,]
test.data <- remaining_data[-vad.index,]
#testing whether total rows are correct
nrow(test.data) + nrow(vad.data) + nrow(train.data) == nrow(credit)
set.seed(101)
acc3 <- data.frame(matrix(nrow = 30 ,ncol = 2))
names(acc3) <- c("k" , "accuracy")
pred7<- rep(0,(nrow(vad.data)))
for(n in 1:30){
knn_model3=kknn(R1~., train = train.data,test = vad.data,k=n, scale = T)
res7 <- sum(knn_model3$fitted.values == vad.data$R1) / nrow(vad.data)
acc3[n,1]<- n
acc3[n,2]<- res7
}
head(acc3[order(-acc3["accuracy"]),])
knn.plt3 <-ggplot(acc3, aes(x = k , y = accuracy)) + geom_point() + geom_line(lty = "dotted" , color = "red")
knn.plt3
tinytex::install_tinytex()
install.packages('tinytex')
install.packages("tinytex")
install.packages(c("backports", "haven", "tidyr"))
library(tinytex)
system("defaults write org.R-project.R force.LANG en_US.UTF-8")
Sys.setenv(LANG = "en")
127
+1
y+1
y+1
tinytex::install_tinytex()
if tinytex:::is_tinytex() is TRUE
if tinytex:::is_tinytex()
Sys.getlocale()
Sys.setlocale("LC_ALL","English")
install.packages("AER")
install.packages("stargrazer")
install.packages("stargrazer")
install.packages("lmtest")
library("AER")
library("stargazer")
install.packages("stargazer")
library("dyplr")
library("dplyr")
library(lmtest)
#load data set into workspace
#from package "AER"
data("CASchools")
rm(list = ls())
source('~/.active-rstudio-document', echo=TRUE)
class(CAschools)
class(CASchools)
#look at data set
head(CASchools)
view(CASchools)
CASchools
#SUmmary stat of STR and score
#select in dplyr package
CASchools %>%
select(STR,score)%>%
summary()
#create new variables using dplyr
#CASchools %>%
CASchools %>%
mutate( STR = students / teachers ) %>%
mutate( score = (read + math) / 2 ) -> CASchools
summary(CASchools)
#SUmmary stat of STR and score
#select in dplyr package
CASchools %>%
select(STR,score)%>%
summary()
source('~/.active-rstudio-document', echo=TRUE)
stargazer(CASchools , type = "text")
source('D:/ernie/waseda/2020春学期/Econometrics and Data Analysis using R/lecture code/regression 2.R', echo=TRUE)
#scatter plot
plot(score - STR,
data = CASchools,
main = "Scatterplot of TestScore and STR",
xlab = "STR (X)",
ylab = "Test Score (Y)")
source('D:/ernie/waseda/2020春学期/Econometrics and Data Analysis using R/lecture code/regression 2.R', echo=TRUE)
#scatter plot
plot(score ~ STR,
data = CASchools,
main = "Scatterplot of TestScore and STR",
xlab = "STR (X)",
ylab = "Test Score (Y)")
source('D:/ernie/waseda/2020春学期/Econometrics and Data Analysis using R/lecture code/regression 2.R', echo=TRUE)
source('D:/ernie/waseda/2020春学期/Econometrics and Data Analysis using R/lecture code/regression 2.R', echo=TRUE)
cor(CASchools$STR ,CASchools$score)
source('D:/ernie/waseda/2020春学期/Econometrics and Data Analysis using R/lecture code/regression 2.R', echo=TRUE)
source('D:/ernie/waseda/2020春学期/Econometrics and Data Analysis using R/lecture code/regression 2.R', echo=TRUE)
source('D:/ernie/self-study/GTxMicroMasters/Introduction to Analytics Modeling/week5/homework/week-5-homework.R', echo=TRUE)
#choosing coefficients
coef(cv.lasso, s = "lambda.min")
help(coef)
View(train)
select.ind = which(coef(cv.lasso, s = "lambda.min") != 0)
select.ind = select.ind[-1]-1 # remove Intercept
select.ind # 第幾個變數是重要的 (不看 `Intercept`)
colnames(train)
train[select.ind]
colnames(train[select.ind])
important <- colnames(train[select.ind])
#Regression model
lm(crime ~., data = train[,important])
#Regression model
lm(Crime ~., data = train[,important])
View(train)
#Regression model
lm(Crime~., data = train[,important])
predict.results
#Results
stargazer(step.both.1,step.both.2 ,backward.sel,forward.sel, type = "text")
#Regression model
lm(Crime~., data = train)
#Regression model
lm(Crime~., data = train[,c(important)])
#Regression model
lm(Crime~., data = train[,c(important,"Crime")])
#Regression model
lasso.reg <- lm(Crime~., data = train[,c(important,"Crime")])
summary(lasso.reg)
predict(lasso.reg,test)
#Results
stargazer(step.both.1,step.both.2 ,backward.sel,forward.sel,lasso.reg type = "text")
#Results
stargazer(step.both.1,step.both.2 ,backward.sel,forward.sel,lasso.reg, type = "text")
res.lasso <- test %>%
add_predictions(.,lasso.reg) %>%
select('observations' = Crime, pred) %>%
as.data.frame()
Lasso.regression <- postResample(obs = res.lasso.1$observations, pred = res.lasso.1$pred)
res.lasso <- test %>%
add_predictions(.,lasso.reg) %>%
select('observations' = Crime, pred) %>%
as.data.frame()
Lasso.regression <- postResample(obs = res.lasso.1$observations, pred = res.lasso.1$pred)
Lasso.regression <- postResample(obs = res.lasso$observations, pred = res.lasso$pred)
Lasso.regression
res.lasso
Lasso.regression <- postResample(obs = res.lasso$observations, pred = res.lasso$pred)
stargazer(res.lasso, type = "txt")
stargazer(res.lasso, type = "text")
stargazer(res.lasso, )
stargazer(res.lasso)
stargazer(res.lasso, type = "latex" , results = asis)
stargazer(res.lasso, type = "latex" , results = 'asis')
stargazer(res.lasso, type = "text")
stargazer(res.lasso, type = "text"  )
Lasso.regression <- postResample(obs = res.lasso$observations, pred = res.lasso$pred)
Lasso.regression
predict.results
