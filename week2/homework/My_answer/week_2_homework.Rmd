---
title: "week2 homework"
author: "Chen Yi-Ju(Ernie)"
date: "`r Sys.Date()`"
output: 
  #html_document: default
  pdf_document:
    latex_engine: xelatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Question 4.1
### Describe a situation or problem from your job, everyday life, current events, etc., for which a clustering model would be appropriate. List some (up to 5) predictors that you might use.<br>

<font size="4"> A clustering model can be seen in the case of industry clusters, which means that certain kind of factories/businesses may be more likely to choose to locate at a certain country or city. For example, toy factories tend to be located in places with cheaper wages. <font/>
Possible factors should include: <br>
labor wages, land prices , education level , tax levels

## Question 4.2
###   Use the R function kmeans to cluster the points as well as possible. Report the best combination of predictors, your suggested value of k, and how well your best clustering predicts flower type.<br>
  The iris data set iris.txt contains 150 data points, each with four predictor variables and one categorical response. The predictors are the width and length of the sepal and petal of flowers and the response is the type of flower.<br>

### Answer :I ended up with the conclusion of k = 3 has the highest accuracy of 95%.
### Process
Loading data and libraries
```{r load,echo = TRUE, message = FALSE, warning = FALSE}
#Loading data and library
setwd("D:/ernie/self-study/GTxMicroMasters/Introduction to Analytics Modeling/week2/homework")
iris <- read.table("iris.txt")
library(tidyverse)
library(factoextra)
library(cluster)
```

First of all, a quick overview of the data
```{r data, echo = TRUE , message = TRUE, warning = TRUE}
head(iris)
```

Trying out  k at 3  for a model
```{r model,echo = TRUE, message = FALSE, warning = FALSE}
set.seed(101)
irisCluster <- kmeans(iris[,1:4], 3, nstart = 20)
comparison <-table(irisCluster$cluster, iris$Species)
comparison
```
We can see that the accuracy is quite high, with only a slight mix between versicolor and virginica.
```{r cluspot}
clusplot(iris, irisCluster$cluster , color = T, shade = T, labels = 0,lines = 0)
```

But to make sure of which k to use, we try out the elbow method:
```{r elbow,echo = TRUE, message = FALSE, warning = FALSE}
test_mat<- data.frame(matrix(nrow = 15, ncol = 2,0))
for (k in 1:15){
  irisCluster_test <- kmeans(iris[,1:4], k, nstart = 20)
  j <- irisCluster_test$tot.withinss
  test_mat[k,1] <- k
  test_mat[k,2] <- j
}
colnames(test_mat) <- c("k" , "Total")# Total within-cluster sum of squares

```

The results, as in a matrix
```{r matrix , echo = TRUE, message = FALSE, warning= FALSE}
test_mat
```

Graphically represented: We can see that k = 3 is indeed an appropriate choice, due to the great decrease in error and the steadiness onwards.
```{r elbowplot , echo = TRUE, message = FALSE, warning= FALSE}
ggplot(test_mat , aes(x = k , y = Total ))+
  geom_point()+
  geom_line()
```

Lastly, in this case k = 3 is quite obvious to begin with since we already know how many species are there for us to categorize, we can see it even more clearly from this plot.
```{r visualization , echo = TRUE, message = FALSE, warning= FALSE}
#Visualization

pl <- ggplot(iris, aes(Petal.Length ,Petal.Width, color = Species))+
  geom_point(size = 3)
print(pl)
```



### In coclusion, k = 3 is the most accurate model with 95% accuracy

### Question 5.1
##Using crime data from the file uscrime.txt ,test to see whether there are any outliers in the last column (number of crimes per 100,000 people). Use the grubbs.test function in the outliers package in R.<br>
### Answer: Yes and No.<br>
The reason for saying that is that it depends on how you define outliers as to how far it is. It is a "weak outlier" , if one defines p-value to be less than 0.1, but not so if it is put under the stricter definition of p-value = 0.05.<br>
According to the plots and tests, there are outliers far enough well out of 1.5 SD of the data.However, even with the largest of the outliers, they are not far enough (to large of a p-value:0.07) to be strictly statistically<br>

### Process
Loading data
```{r loadData2 , echo = TRUE, message = FALSE, warning= FALSE}
library(outliers)
crime <- read.table("uscrime.txt" , header = T)
crimePerHundred <-crime$Crime
```

Quick Overview
```{r , echo = TRUE, message = FALSE, warning= FALSE}
summary(crimePerHundred)
```
Box-plot show that there are some outliers, outside 1.5SDs of the data.
```{r , echo = TRUE, message = FALSE, warning= FALSE}
ggplot(crime, aes(y = Crime)) + 
  geom_boxplot(outlier.color = "red" ,width = 5)
```

Testing using the grubbs test: It gives us the furthest outlier. But the p-value is slightly too large.
```{r , echo = TRUE, message = FALSE, warning= FALSE}
grubbs.test(crimePerHundred)
```
### Conclusion: There are ouliers under loosely defined conditions.

## Question 6.1
### Describe a situation or problem from your job, everyday life, current events, etc., for which a Change Detection model would be appropriate. Applying the CUSUM technique, how would you choose the critical value and the threshold?<br>

In my country (Taiwan), we have big downpours of rain during monsoon season and typhoon season with the extreme of more than 1000centimeters of rain per 24 hours. Therefore, the water level of rivers and stream would be my choice of applying a Change Detection.<br>
In terms of the C value and threshold. Because in case of an breach of embankment, floods may cause extremely big damage to both property and lives. Therefore, an optimal choice would be a small C and an relatively high threshold as you can never be too careful.

## Question 6.2
### 1. Using July through October daily-high-temperature data for Atlanta for 1996 through 2015, use a CUSUM approach to identify when unofficial summer ends (i.e., when the weather starts cooling off) each year. <br>

### Answer: October-2 is my answer.
### Process
Loading Data
```{r , echo = TRUE, message = FALSE, warning= FALSE}
library(qcc)
library(dplyr)
library(tidyr)
library(lubridate)
#loading and tidyiing data
weather <- data.frame(read.table("temps.txt" , header = T))%>%
  gather(year,temp,-DAY)%>%
  mutate(year = as.factor(year),
         date = paste(DAY,year,sep = "-"))%>%
  mutate(date = dmy(date),
         month = month(date),
         day = day(date))
summary(weather)
```
EDA: box plots of temperture by year 
```{r , echo = TRUE, message = FALSE, warning= FALSE}
#average temperture per year
avg_by_year <-weather %>%
  group_by(year)%>%
  summarise(avg_year = mean(temp))
 pl2 <-ggplot(weather, aes(x = date, y = temp , color = year))+
   geom_boxplot()
```

```{r , echo = TRUE, message = FALSE, warning= FALSE}
pl2
```



EDA2: boxplot of temperature by month
```{r , echo = TRUE, message = FALSE, warning= FALSE}
#average temperture per month
avg_by_month <-weather %>%
  group_by(month)%>%
  summarise(avg_month = mean(temp))
avg_by_month
pl3 <-ggplot(weather, aes(x = month, y = temp,group = month))+
  geom_boxplot()
```

```{r , echo = TRUE, message = FALSE, warning= FALSE}
pl3
```

The Cusum Process
```{r , echo = TRUE, message = FALSE, warning= FALSE}
#cusum
weather_2 <- data.frame(read.table("temps.txt" , header = T)) 
day_mean <- rowMeans(weather_2[,-1])
summary(day_mean)
```
Setting C as 4, which is about half way between the 1st Quarter and the median
```{r , echo = TRUE, message = FALSE, warning= FALSE}
# mean - Xi (want to find decrease)
total_mean<- mean(day_mean)
diff <- total_mean - day_mean %>%
  data.frame()
#setting C
C <- 4 #half way between median and 1st quarter
diff <- diff - C
diff<- mutate(diff, date = weather_2$DAY)
colnames(diff)<- c("Xi-m-C" , "date")
```
Deriving the St
```{r , echo = TRUE, message = FALSE, warning= FALSE}
S <- matrix(nrow = 124,0)

for (i in 1:123){
  if (i == 1){
    S[i+1,1] <- max(0,S[1,1]+diff[i,1])
  }else{
    S[i+1,1] <- max(0,S[i,1]+diff[i,1])
  }
  
}
S<- data.frame(S)
diff <- diff%>%
  mutate(S = S[2:124,1])
```

```{r , echo = TRUE, message = FALSE, warning= FALSE}
head(diff)
```
Graphically:
```{r , echo = TRUE, message = FALSE, warning= FALSE}
pl4 <- ggplot(diff, aes(x= seq_along(diff[,2]), y = diff[,3] ))+
        geom_point()+
        geom_line()
pl4
```



I set the threshold at 10:
```{r , echo = TRUE, message = FALSE, warning= FALSE}
#threshold set at 10
S2 <-subset(diff,S>10)
S2
```
### Conclusion : Oct-2.

### 2. Use a CUSUM approach to make a judgment of whether Atlantaâ€™s summer climate has gotten warmer in that time (and if so, when). <br>

### Answer : Yes, from 2010.
### Process:

Loading Data  + setting C and threshold
```{r , echo = TRUE, message = FALSE, warning= FALSE}
ann_data<-weather%>% 
        group_by(year)%>%
         summarise(year_avg = mean(temp))
year_mean<- mean(ann_data$year_avg)
sd(ann_data$year_avg)
#setting C
C2 <- 1
thresh2 <- 2#total of C and threshold would be approx. 2 Sd. away from mean
```

deriving S
```{r , echo = TRUE, message = FALSE, warning= FALSE}
ann_data <-ann_data %>%
  mutate(diff = year_avg - year_mean-C2)%>%
  data.frame()
S2 <-matrix(nrow = 21,0)
for (i in 1:20){
  if (i == 1){
    S2[i+1,1] <- max(0,S2[1,1]+ann_data[i,3])
  }else{
    S2[i+1,1] <- max(0,S2[i,1]+ann_data[i,3])
  }
  
}
```

graphically represented:
```{r , echo = TRUE, message = FALSE, warning= FALSE}
S2 <- data.frame(S2)
ann_data <-ann_data %>%
        mutate(S = S2[2:21,])
pl5 <- ggplot(ann_data, aes(x= year, y = S , group = 1))+
  geom_point()+
  geom_line()
```

```{r , echo = TRUE, message = FALSE, warning= FALSE}
pl5
```




The earliest year to cross the threshold is 2010.
```{r , echo = TRUE, message = FALSE, warning= FALSE}
subset(ann_data, S > thresh2)
```

### Conclusion: The temperature has been rising since 2010
